{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10506414,"sourceType":"datasetVersion","datasetId":6504252},{"sourceId":235369,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":201042,"modelId":222852}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom datasets import load_dataset\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T17:17:09.702412Z","iopub.execute_input":"2025-01-18T17:17:09.702733Z","iopub.status.idle":"2025-01-18T17:17:32.979886Z","shell.execute_reply.started":"2025-01-18T17:17:09.702703Z","shell.execute_reply":"2025-01-18T17:17:32.979120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"csv_file = \"/kaggle/input/synthetic-data/Generated_Synthetic_Prompt-Completion_Dataset.csv\"  # Replace with your file path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T17:20:43.021317Z","iopub.execute_input":"2025-01-18T17:20:43.021607Z","iopub.status.idle":"2025-01-18T17:20:43.025379Z","shell.execute_reply.started":"2025-01-18T17:20:43.021586Z","shell.execute_reply":"2025-01-18T17:20:43.024441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = load_dataset(\"csv\", data_files=csv_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T17:20:44.972379Z","iopub.execute_input":"2025-01-18T17:20:44.972660Z","iopub.status.idle":"2025-01-18T17:21:17.119188Z","shell.execute_reply.started":"2025-01-18T17:20:44.972640Z","shell.execute_reply":"2025-01-18T17:21:17.118528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Prepare the tokenizer and model\nmodel_name = \"/kaggle/input/gpt2/pytorch/default/1/gpt2\"  # Replace with your preferred pre-trained model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T17:21:39.187792Z","iopub.execute_input":"2025-01-18T17:21:39.188170Z","iopub.status.idle":"2025-01-18T17:21:39.775243Z","shell.execute_reply.started":"2025-01-18T17:21:39.188139Z","shell.execute_reply":"2025-01-18T17:21:39.774037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add a padding token if it's missing\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add a custom padding token\n    model.resize_token_embeddings(len(tokenizer))  # Resize model embeddings to include the new token\n\n# Ensure the pad_token is set\ntokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else '[PAD]'\n\n# Define the preprocess function\ndef preprocess_function(examples):\n    inputs = [f\"Prompt: {prompt}\\nCompletion: {completion}\" \n              for prompt, completion in zip(examples[\"prompt\"], examples[\"completion\"])]\n    model_inputs = tokenizer(\n        inputs, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n    )\n    labels = model_inputs[\"input_ids\"].clone()\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\n# Map the preprocess function to the dataset\ntokenized_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T17:23:45.748627Z","iopub.execute_input":"2025-01-18T17:23:45.748918Z","iopub.status.idle":"2025-01-18T17:23:46.976502Z","shell.execute_reply.started":"2025-01-18T17:23:45.748896Z","shell.execute_reply":"2025-01-18T17:23:46.975672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\ntrain_dataset = split_dataset[\"train\"]\neval_dataset = split_dataset[\"test\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T17:24:35.697072Z","iopub.execute_input":"2025-01-18T17:24:35.697376Z","iopub.status.idle":"2025-01-18T17:24:35.710458Z","shell.execute_reply.started":"2025-01-18T17:24:35.697354Z","shell.execute_reply":"2025-01-18T17:24:35.709614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    report_to=\"none\"  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T17:28:53.638435Z","iopub.execute_input":"2025-01-18T17:28:53.638759Z","iopub.status.idle":"2025-01-18T17:28:53.668054Z","shell.execute_reply.started":"2025-01-18T17:28:53.638734Z","shell.execute_reply":"2025-01-18T17:28:53.667421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    tokenizer=tokenizer,   \n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T17:28:56.320767Z","iopub.execute_input":"2025-01-18T17:28:56.321093Z","iopub.status.idle":"2025-01-18T17:35:45.801931Z","shell.execute_reply.started":"2025-01-18T17:28:56.321066Z","shell.execute_reply":"2025-01-18T17:35:45.801068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the model and tokenizer\noutput_dir = \"/kaggle/working/fine_tuned_model\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model and tokenizer saved to {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T17:38:16.841355Z","iopub.execute_input":"2025-01-18T17:38:16.841687Z","iopub.status.idle":"2025-01-18T17:38:17.805958Z","shell.execute_reply.started":"2025-01-18T17:38:16.841662Z","shell.execute_reply":"2025-01-18T17:38:17.805095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the model\nfrom transformers import pipeline\n\n# Load the model and tokenizer\nmodel_path = \"/kaggle/working/fine_tuned_model\"\nfine_tuned_model = AutoModelForCausalLM.from_pretrained(model_path)\nfine_tuned_tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Create a text generation pipeline\ngenerator = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=fine_tuned_tokenizer)\n\n# Generate a response\nprompt = \"Swap 7381 USDC for ETH on Pangolin on Ethereum.\"\nresponse = generator(prompt, max_length=100, num_return_sequences=1)\nprint(response[0]['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T18:32:44.042696Z","iopub.execute_input":"2025-01-18T18:32:44.042980Z","iopub.status.idle":"2025-01-18T18:32:45.059721Z","shell.execute_reply.started":"2025-01-18T18:32:44.042959Z","shell.execute_reply":"2025-01-18T18:32:45.058906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}